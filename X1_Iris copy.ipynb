{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Analysis on Iris Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and set random seeds\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# GPU settings if using CUDA\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import clustering algorithms and utility functions\n",
    "from pso import ParticleSwarmOptimizedClustering\n",
    "from apso import AdaptiveParticleSwarmOptimizedClustering\n",
    "from particle import quantization_error, calc_sse\n",
    "from utils import normalize\n",
    "from kmeans import KMeans\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score\n",
    "import random\n",
    "random.seed(2024)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the seeds dataset\n",
    "data = pd.read_csv('seed.txt', sep='\\t', header=None)\n",
    "y = data[7]      # Extract labels\n",
    "x = data.drop([7], axis=1)      # Extract features\n",
    "\n",
    "x = x.values\n",
    "x = normalize(x)    # Normalize features\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.values\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means++ Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# K-Means++ clustering with 10 runs\n",
    "kmeanspp = {\n",
    "    'silhouette': [],\n",
    "    'sse': [],\n",
    "    'quantization': [],\n",
    "    'sse_history': [] \n",
    "}\n",
    "\n",
    "max_attempts = 1  # Maximum number of attempts to get a valid clustering\n",
    "\n",
    "for _ in range(10):\n",
    "    attempt = 0\n",
    "    while attempt < max_attempts:\n",
    "        kmean_rep = KMeans(n_cluster=3, init_pp=True)\n",
    "        kmean_rep.fit(x)\n",
    "        predicted_kmean_rep = kmean_rep.predict(x)\n",
    "        \n",
    "        unique_labels = np.unique(predicted_kmean_rep)\n",
    "        if len(unique_labels) > 1:\n",
    "            # Valid clustering found\n",
    "            break\n",
    "        attempt += 1\n",
    "    \n",
    "    if attempt == max_attempts:\n",
    "        print(f\"Failed to find a valid clustering after {max_attempts} attempts. Skipping this iteration.\")\n",
    "        continue\n",
    "    \n",
    "    silhouette = silhouette_score(x, predicted_kmean_rep)\n",
    "    sse = kmean_rep.SSE\n",
    "    quantization = quantization_error(centroids=kmean_rep.centroid, data=x, labels=predicted_kmean_rep)\n",
    "    kmeanspp['silhouette'].append(silhouette)\n",
    "    kmeanspp['sse'].append(sse)\n",
    "    kmeanspp['quantization'].append(quantization)\n",
    "    kmeanspp['sse_history'].append([sse])  \n",
    "\n",
    "print(\"K-Means++ clustering completed. Raw data collected for further analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PSO Clustering Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score\n",
    "# Standard PSO clustering with 10 runs\n",
    "pso_plain = {\n",
    "    'silhouette': [],\n",
    "    'sse': [],\n",
    "    'quantization': [],\n",
    "    'sse_history': []\n",
    "}\n",
    "\n",
    "max_attempts = 1  # Maximum number of attempts to get a valid clustering\n",
    "\n",
    "for _ in range(10):\n",
    "    attempt = 0\n",
    "    while attempt < max_attempts:\n",
    "        pso_rep = ParticleSwarmOptimizedClustering(\n",
    "            n_cluster=3, n_particles=15, data=x, hybrid=False, max_iter=1000, print_debug=1000)\n",
    "        sse_history = pso_rep.run() \n",
    "        pso_kmeans = KMeans(n_cluster=3, init_pp=False, seed=2024)\n",
    "        pso_kmeans.centroid = pso_rep.gbest_centroids.copy()\n",
    "        predicted_pso_rep = pso_kmeans.predict(x)\n",
    "        \n",
    "        unique_labels = np.unique(predicted_pso_rep)\n",
    "        if len(unique_labels) > 1:\n",
    "            # Valid clustering found\n",
    "            break\n",
    "        attempt += 1\n",
    "    \n",
    "    if attempt == max_attempts:\n",
    "        print(f\"Failed to find a valid clustering after {max_attempts} attempts. Skipping this iteration.\")\n",
    "        continue\n",
    "    \n",
    "    sse = calc_sse(centroids=pso_rep.gbest_centroids, data=x, labels=predicted_pso_rep)\n",
    "    quantization = pso_rep.gbest_score\n",
    "    silhouette = silhouette_score(x, predicted_pso_rep)\n",
    "    pso_plain['silhouette'].append(silhouette)\n",
    "    pso_plain['sse'].append(sse) \n",
    "    pso_plain['quantization'].append(quantization)\n",
    "    pso_plain['sse_history'].append(sse_history)\n",
    "\n",
    "print(\"PSO Plain clustering completed. Raw data collected for further analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid PSO Clustering Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score\n",
    "# Hybrid PSO clustering with 10 runs\n",
    "pso_hybrid = {\n",
    "    'silhouette': [],\n",
    "    'sse': [],\n",
    "    'quantization': [],\n",
    "    'sse_history': []\n",
    "}\n",
    "\n",
    "max_attempts = 1  # Maximum number of attempts to get a valid clustering\n",
    "\n",
    "for _ in range(10):\n",
    "    attempt = 0\n",
    "    while attempt < max_attempts:\n",
    "        pso_rep = ParticleSwarmOptimizedClustering(\n",
    "            n_cluster=3, n_particles=15, data=x, hybrid=True, max_iter=1000, print_debug=1000)\n",
    "        sse_history = pso_rep.run()  \n",
    "        pso_kmeans = KMeans(n_cluster=3, init_pp=False, seed=2024)\n",
    "        pso_kmeans.centroid = pso_rep.gbest_centroids.copy()\n",
    "        predicted_pso_rep = pso_kmeans.predict(x)\n",
    "        \n",
    "        unique_labels = np.unique(predicted_pso_rep)\n",
    "        if len(unique_labels) > 1:\n",
    "            # Valid clustering found\n",
    "            break\n",
    "        attempt += 1\n",
    "    \n",
    "    if attempt == max_attempts:\n",
    "        print(f\"Failed to find a valid clustering after {max_attempts} attempts. Skipping this iteration.\")\n",
    "        continue\n",
    "    \n",
    "    sse = calc_sse(centroids=pso_rep.gbest_centroids, data=x, labels=predicted_pso_rep)\n",
    "    quantization = pso_rep.gbest_score\n",
    "    silhouette = silhouette_score(x, predicted_pso_rep)\n",
    "    pso_hybrid['silhouette'].append(silhouette)\n",
    "    pso_hybrid['sse'].append(sse)\n",
    "    pso_hybrid['quantization'].append(quantization)\n",
    "    pso_hybrid['sse_history'].append(sse_history)\n",
    "\n",
    "print(\"PSO Hybrid clustering completed. Raw data collected for further analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APSO Implementations (Plain and Hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plain APSO clustering with 10 runs\n",
    "apso_plain = {\n",
    "    'silhouette': [],\n",
    "    'sse': [],\n",
    "    'quantization': [],\n",
    "    'sse_history': []\n",
    "}\n",
    "\n",
    "max_attempts = 1  # Maximum number of attempts to get a valid clustering\n",
    "\n",
    "for _ in range(10):\n",
    "    attempt = 0\n",
    "    while attempt < max_attempts:\n",
    "        apso_rep = AdaptiveParticleSwarmOptimizedClustering(\n",
    "            n_cluster=3, n_particles=15, data=x, hybrid=False, max_iter=1000, print_debug=1000)\n",
    "        sse_history = apso_rep.run() \n",
    "        apso_kmeans = KMeans(n_cluster=3, init_pp=False, seed=2024)\n",
    "        apso_kmeans.centroid = apso_rep.gbest_centroids.copy()\n",
    "        predicted_apso_rep = apso_kmeans.predict(x)\n",
    "        \n",
    "        unique_labels = np.unique(predicted_apso_rep)\n",
    "        if len(unique_labels) > 1:\n",
    "            # Valid clustering found\n",
    "            break\n",
    "        attempt += 1\n",
    "    \n",
    "    if attempt == max_attempts:\n",
    "        print(f\"Failed to find a valid clustering after {max_attempts} attempts. Skipping this iteration.\")\n",
    "        continue\n",
    "    \n",
    "    sse = calc_sse(centroids=apso_rep.gbest_centroids, data=x, labels=predicted_apso_rep)\n",
    "    quantization = apso_rep.gbest_score\n",
    "    silhouette = silhouette_score(x, predicted_apso_rep)\n",
    "    apso_plain['silhouette'].append(silhouette)\n",
    "    apso_plain['sse'].append(sse)\n",
    "    apso_plain['quantization'].append(quantization)\n",
    "    apso_plain['sse_history'].append(sse_history)\n",
    "\n",
    "print(\"APSO Plain clustering completed. Raw data collected for further analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APSO Hybrid\n",
    "apso_hybrid = {\n",
    "    'silhouette': [],\n",
    "    'sse': [],\n",
    "    'quantization': [],\n",
    "    'sse_history': []\n",
    "}\n",
    "\n",
    "for _ in range(10):\n",
    "    attempt = 0\n",
    "    while attempt < max_attempts:\n",
    "        apso_rep = AdaptiveParticleSwarmOptimizedClustering(\n",
    "            n_cluster=3, n_particles=15, data=x, hybrid=True, max_iter=1000, print_debug=1000)\n",
    "        sse_history = apso_rep.run()  # Now returns SSE history\n",
    "        apso_kmeans = KMeans(n_cluster=3, init_pp=False, seed=2024)\n",
    "        apso_kmeans.centroid = apso_rep.gbest_centroids.copy()\n",
    "        predicted_apso_rep = apso_kmeans.predict(x)\n",
    "        \n",
    "        unique_labels = np.unique(predicted_apso_rep)\n",
    "        if len(unique_labels) > 1:\n",
    "            # Valid clustering found\n",
    "            break\n",
    "        attempt += 1\n",
    "    \n",
    "    if attempt == max_attempts:\n",
    "        print(f\"Failed to find a valid clustering after {max_attempts} attempts. Skipping this iteration.\")\n",
    "        continue\n",
    "    \n",
    "    sse = calc_sse(centroids=apso_rep.gbest_centroids, data=x, labels=predicted_apso_rep)\n",
    "    quantization = apso_rep.gbest_score\n",
    "    silhouette = silhouette_score(x, predicted_apso_rep)\n",
    "    apso_hybrid['silhouette'].append(silhouette)\n",
    "    apso_hybrid['sse'].append(sse)\n",
    "    apso_hybrid['quantization'].append(quantization)\n",
    "    apso_hybrid['sse_history'].append(sse_history)\n",
    "\n",
    "print(\"APSO Hybrid clustering completed. Raw data collected for further analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Algorithm Performance Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_statistics(data):\n",
    "    return {\n",
    "        'mean': np.around(np.mean(data), decimals=10),\n",
    "        'stdev': np.around(np.std(data), decimals=10),\n",
    "        'min': np.around(np.min(data), decimals=10),\n",
    "        'max': np.around(np.max(data), decimals=10),\n",
    "        'percentile_75': np.around(np.percentile(data, 75), decimals=10),\n",
    "        'valid_count': np.sum(~np.isnan(data))\n",
    "    }\n",
    "\n",
    "methods = ['K-Means++', 'PSO', 'PSO Hybrid', 'APSO', 'APSO Hybrid']\n",
    "metrics = ['sse', 'silhouette', 'quantization']\n",
    "results = [kmeanspp, pso_plain, pso_hybrid, apso_plain, apso_hybrid]\n",
    "\n",
    "benchmark = {'method': methods}\n",
    "\n",
    "for i, method in enumerate(methods):\n",
    "    valid_runs = np.sum(~np.isnan(results[i][metrics[0]]))  # Assuming if SSE is valid, the run is valid\n",
    "    success_rate = valid_runs / 10  # 10 iterations\n",
    "    benchmark['success_rate'] = benchmark.get('success_rate', []) + [success_rate]\n",
    "    \n",
    "    for metric in metrics:\n",
    "        stats = calculate_statistics(results[i][metric])\n",
    "        for stat, value in stats.items():\n",
    "            if stat != 'valid_count':  # We don't need to include this in the final DataFrame\n",
    "                benchmark[f'{metric}_{stat}'] = benchmark.get(f'{metric}_{stat}', []) + [value]\n",
    "\n",
    "benchmark_df = pd.DataFrame.from_dict(benchmark)\n",
    "\n",
    "# Reorder columns for better readability\n",
    "column_order = ['method', 'success_rate'] + [f'{metric}_{stat}' for metric in metrics for stat in ['mean', 'stdev', 'min', 'max', 'percentile_75']]\n",
    "benchmark_df = benchmark_df[column_order]\n",
    "\n",
    "\n",
    "benchmark_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence Analysis Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "methods = ['K-Means++', 'PSO', 'PSO Hybrid', 'APSO', 'APSO Hybrid']\n",
    "\n",
    "# Function to plot convergence of global best scores\n",
    "def plot_gbest_convergence(results_dict, benchmark_df):\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    sns.set_style(\"darkgrid\")\n",
    "    sns.set_palette(\"husl\")\n",
    "    \n",
    "    methods = ['PSO', 'PSO Hybrid', 'APSO', 'APSO Hybrid']\n",
    "    \n",
    "    all_gbest_values = []\n",
    "    for method in methods:\n",
    "        gbest_histories = results_dict[method]['sse_history'] \n",
    "        max_length = max(len(history) for history in gbest_histories)\n",
    "        padded_histories = [history + [history[-1]]*(max_length - len(history)) for history in gbest_histories]\n",
    "        mean_gbest_history = np.mean(padded_histories, axis=0)\n",
    "        std_gbest_history = np.std(padded_histories, axis=0)\n",
    "        \n",
    "        all_gbest_values.extend(mean_gbest_history)\n",
    "        \n",
    "        iterations = range(len(mean_gbest_history))\n",
    "        plt.plot(iterations, mean_gbest_history, label=method, linewidth=2.5)\n",
    "        plt.fill_between(iterations, mean_gbest_history - std_gbest_history, \n",
    "                         mean_gbest_history + std_gbest_history, alpha=0.2)\n",
    "    \n",
    "    plt.title('Convergence of Global Best Scores', fontsize=24, fontweight='bold', pad=20)\n",
    "    plt.xlabel('Iterations', fontsize=18, labelpad=15)\n",
    "    plt.ylabel('Global Best Score', fontsize=18, labelpad=15)\n",
    "    plt.legend(fontsize=14, title='Methods', title_fontsize=16, loc='upper right', \n",
    "               bbox_to_anchor=(1.15, 1), borderaxespad=0.)\n",
    "    \n",
    "    plt.xlim(0, max_length)\n",
    "    \n",
    "    # Set y-axis limits to focus on the relevant range\n",
    "    min_gbest = min(all_gbest_values)\n",
    "    max_gbest = max(all_gbest_values)\n",
    "    gbest_range = max_gbest - min_gbest\n",
    "    plt.ylim(min_gbest - 0.05 * gbest_range, max_gbest + 0.05 * gbest_range)\n",
    "    \n",
    "    plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    \n",
    "    # Add annotations\n",
    "    best_method = benchmark_df.loc[benchmark_df['quantization_mean'].idxmin(), 'method']\n",
    "    best_value = benchmark_df['quantization_mean'].min()\n",
    "    plt.annotate(f'Best method: {best_method}\\nLowest Global Best: {best_value:.4f}', \n",
    "                 xy=(0.02, 0.98), xycoords='axes fraction', va='top',\n",
    "                 fontsize=14, bbox=dict(boxstyle=\"round,pad=0.5\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Usage\n",
    "plot_gbest_convergence({\n",
    "    'PSO': pso_plain,\n",
    "    'PSO Hybrid': pso_hybrid,\n",
    "    'APSO': apso_plain,\n",
    "    'APSO Hybrid': apso_hybrid\n",
    "}, benchmark_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_performance_comparison(benchmark_df):\n",
    "    metrics = ['sse', 'silhouette', 'quantization']\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        mean_col = f'{metric}_mean'\n",
    "        std_col = f'{metric}_stdev'\n",
    "        \n",
    "        # Create the bar plot\n",
    "        sns.barplot(x='method', y=mean_col, data=benchmark_df, ax=axes[i], \n",
    "                    capsize=0.1, color='skyblue', alpha=0.8)\n",
    "        \n",
    "        # Add error bars\n",
    "        axes[i].errorbar(x=range(len(benchmark_df)), y=benchmark_df[mean_col], \n",
    "                         yerr=benchmark_df[std_col], fmt='none', color='black', capsize=5)\n",
    "        \n",
    "        axes[i].set_title(f'Comparison of {metric.upper()}')\n",
    "        axes[i].set_xlabel('')\n",
    "        axes[i].set_xticklabels(axes[i].get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Usage\n",
    "plot_performance_comparison(benchmark_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stability Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "\n",
    "def enhanced_cluster_stability_analysis(data, methods, n_runs=5):\n",
    "    stability_scores = {method: [] for method in methods}\n",
    "    \n",
    "    for method in methods:\n",
    "        labels_list = []\n",
    "        for _ in range(n_runs):\n",
    "            if method == 'K-Means++':\n",
    "                kmeans = KMeans(n_clusters=3, init='k-means++', random_state=None)\n",
    "                kmeans.fit(data)\n",
    "                labels = kmeans.predict(data)\n",
    "            else:\n",
    "                if 'PSO' in method:\n",
    "                    pso = ParticleSwarmOptimizedClustering(n_cluster=3, n_particles=15, data=data, hybrid=('Hybrid' in method))\n",
    "                else:  # APSO\n",
    "                    pso = AdaptiveParticleSwarmOptimizedClustering(n_cluster=3, n_particles=15, data=data, hybrid=('Hybrid' in method))\n",
    "                pso.run()\n",
    "                labels = pairwise_distances_argmin(data, pso.gbest_centroids)\n",
    "            labels_list.append(labels)\n",
    "        \n",
    "        # Calculate pairwise ARI scores\n",
    "        n = len(labels_list)\n",
    "        ari_scores = []\n",
    "        for i in range(n):\n",
    "            for j in range(i+1, n):\n",
    "                ari = adjusted_rand_score(labels_list[i], labels_list[j])\n",
    "                ari_scores.append(ari)\n",
    "        \n",
    "        stability_scores[method] = ari_scores\n",
    "    \n",
    "    # Plot stability scores\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    sns.set_palette(\"viridis\", n_colors=len(methods))\n",
    "    \n",
    "    # Create violin plot\n",
    "    violin_parts = plt.violinplot([stability_scores[method] for method in methods], showmeans=True, showextrema=True, showmedians=True)\n",
    "    \n",
    "    # Customize violin plot\n",
    "    for pc in violin_parts['bodies']:\n",
    "        pc.set_facecolor('#D43F3A')\n",
    "        pc.set_edgecolor('black')\n",
    "        pc.set_alpha(0.7)\n",
    "    \n",
    "    # Add scatter plot for individual points\n",
    "    for i, method in enumerate(methods):\n",
    "        y = stability_scores[method]\n",
    "        x = np.random.normal(i+1, 0.04, len(y))  # Add jitter to x-axis\n",
    "        plt.scatter(x, y, alpha=0.4, color='#2C3E50')\n",
    "    \n",
    "    plt.title('Cluster Stability Analysis Across Methods', fontsize=22, fontweight='bold', pad=20)\n",
    "    plt.ylabel('Adjusted Rand Index (ARI)', fontsize=18, labelpad=15)\n",
    "    plt.xlabel('Clustering Method', fontsize=18, labelpad=15)\n",
    "    plt.xticks(range(1, len(methods)+1), methods, rotation=45, ha='right', fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    \n",
    "    # Add mean values on top of each violin\n",
    "    means = [np.mean(stability_scores[method]) for method in methods]\n",
    "    for i, mean in enumerate(means):\n",
    "        plt.text(i+1, 1.05, f'Mean: {mean:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=12)\n",
    "    \n",
    "    # Add a text box with explanation\n",
    "    explanation = (\n",
    "        \"The Adjusted Rand Index (ARI) measures the similarity between clusterings.\\n\"\n",
    "        \"Higher values indicate more stable clustering across multiple runs.\\n\"\n",
    "        \"ARI ranges from -1 to 1, where:\\n\"\n",
    "        \"  1: Perfect agreement between clusterings\\n\"\n",
    "        \"  0: Random labeling\\n\"\n",
    "        \" <0: Less agreement than expected by chance\"\n",
    "    )\n",
    "    plt.text(0.98, 0.02, explanation, transform=plt.gca().transAxes, fontsize=12,\n",
    "             verticalalignment='bottom', horizontalalignment='right',\n",
    "             bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.8, edgecolor='#2C3E50'))\n",
    "    \n",
    "    # Highlight the best performing method\n",
    "    best_method = max(methods, key=lambda m: np.mean(stability_scores[m]))\n",
    "    best_mean = np.mean(stability_scores[best_method])\n",
    "    plt.text(0.02, 0.02, f\"Best Method: {best_method}\\nMean ARI: {best_mean:.3f}\", \n",
    "            transform=plt.gca().transAxes,\n",
    "            fontsize=12, \n",
    "            verticalalignment='bottom', \n",
    "            horizontalalignment='left',\n",
    "            bbox=dict(boxstyle='round,pad=0.5', facecolor='#ABEBC6', alpha=0.8, edgecolor='#2C3E50'))\n",
    "    \n",
    "    plt.ylim(-0.1, 1.1)  # Set y-axis limits to show full ARI range\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Usage\n",
    "enhanced_cluster_stability_analysis(x, methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Visualization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, pairwise_distances_argmin\n",
    "\n",
    "def run_clustering(method, x, n_clusters=3, n_particles=15, max_iter=1000):\n",
    "    if method == 'K-Means++':\n",
    "        kmeans = KMeans(n_clusters=n_clusters, init='k-means++', random_state=np.random.randint(10000))\n",
    "        kmeans.fit(x)\n",
    "        labels = kmeans.labels_\n",
    "        centroids = kmeans.cluster_centers_\n",
    "    else:\n",
    "        if 'PSO' in method:\n",
    "            pso = ParticleSwarmOptimizedClustering(n_cluster=n_clusters, n_particles=n_particles, \n",
    "                                                   data=x, hybrid=('Hybrid' in method), max_iter=max_iter)\n",
    "        else:  # APSO\n",
    "            pso = AdaptiveParticleSwarmOptimizedClustering(n_cluster=n_clusters, n_particles=n_particles, \n",
    "                                                           data=x, hybrid=('Hybrid' in method), max_iter=max_iter)\n",
    "        pso.run()\n",
    "        centroids = pso.gbest_centroids\n",
    "        labels = pairwise_distances_argmin(x, centroids)\n",
    "    \n",
    "    unique_labels = np.unique(labels)\n",
    "    if len(unique_labels) < 2:\n",
    "        return None, None, None, None, None  # Invalid clustering\n",
    "    \n",
    "    # Calculate all metrics\n",
    "    silhouette = silhouette_score(x, labels)\n",
    "    sse = np.sum((x - centroids[labels]) ** 2)\n",
    "    quantization_error = np.mean(np.min(np.linalg.norm(x[:, np.newaxis] - centroids, axis=2), axis=1))\n",
    "    \n",
    "    return labels, centroids, silhouette, sse, quantization_error\n",
    "\n",
    "def visualize_seed_clusters(data, methods, labels_dict, centroids_dict):\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    sns.set_palette(\"deep\")\n",
    "    \n",
    "    fig = plt.figure(figsize=(25, 30))\n",
    "    fig.suptitle(\"Comparison of Best Clustering Results (5 runs)\", fontsize=24, fontweight='bold', y=0.95)\n",
    "    \n",
    "    color_map = plt.cm.get_cmap('viridis')(np.linspace(0, 1, 3))\n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    data_2d = pca.fit_transform(data)\n",
    "    \n",
    "    for i, method in enumerate(methods):\n",
    "        ax_3d = fig.add_subplot(5, 2, 2*i+1, projection='3d')\n",
    "        ax_2d = fig.add_subplot(5, 2, 2*i+2)\n",
    "        \n",
    "        labels = labels_dict[method]\n",
    "        centroids = centroids_dict[method]\n",
    "        \n",
    "        # 3D visualization (using first 3 features)\n",
    "        for j in range(3):  # Assuming 2 clusters\n",
    "            mask = labels == j\n",
    "            ax_3d.scatter(data[mask, 0], data[mask, 1], data[mask, 2], \n",
    "                          c=[color_map[j]], label=f'Cluster {j}', alpha=0.7)\n",
    "        ax_3d.scatter(centroids[:, 0], centroids[:, 1], centroids[:, 2], \n",
    "                      color='red', s=200, marker='X', label='Centroids')\n",
    "        ax_3d.set_title(f'{method} - 3D', fontsize=18, fontweight='bold')\n",
    "        ax_3d.set_xlabel('Feature 1', fontsize=12)\n",
    "        ax_3d.set_ylabel('Feature 2', fontsize=12)\n",
    "        ax_3d.set_zlabel('Feature 3', fontsize=12)\n",
    "        ax_3d.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "        \n",
    "        # PCA visualization\n",
    "        for j in range(3):  # Assuming 2 clusters\n",
    "            mask = labels == j\n",
    "            ax_2d.scatter(data_2d[mask, 0], data_2d[mask, 1], \n",
    "                          c=[color_map[j]], label=f'Cluster {j}', alpha=0.7)\n",
    "        centroids_2d = pca.transform(centroids)\n",
    "        ax_2d.scatter(centroids_2d[:, 0], centroids_2d[:, 1], \n",
    "                      color='red', s=200, marker='X', label='Centroids')\n",
    "        ax_2d.set_title(f'{method} - PCA', fontsize=18, fontweight='bold')\n",
    "        ax_2d.set_xlabel('First Principal Component', fontsize=12)\n",
    "        ax_2d.set_ylabel('Second Principal Component', fontsize=12)\n",
    "        ax_2d.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "# Run clustering methods multiple times\n",
    "methods = ['K-Means++', 'PSO', 'PSO Hybrid', 'APSO', 'APSO Hybrid']\n",
    "n_runs = 10\n",
    "max_attempts = 5\n",
    "\n",
    "results = {method: [] for method in methods}\n",
    "\n",
    "for method in methods:\n",
    "    valid_runs = 0\n",
    "    attempts = 0\n",
    "    while valid_runs < n_runs and attempts < max_attempts:\n",
    "        cluster_results = run_clustering(method, x)\n",
    "        if cluster_results[0] is not None:  # Check if labels exist\n",
    "            results[method].append(cluster_results)\n",
    "            valid_runs += 1\n",
    "        attempts += 1\n",
    "    \n",
    "    if valid_runs < n_runs:\n",
    "        print(f\"Warning: {method} only produced {valid_runs} valid clusterings out of {attempts} attempts.\")\n",
    "\n",
    "# Select best run for each method using all metrics\n",
    "labels_dict = {}\n",
    "centroids_dict = {}\n",
    "\n",
    "for method in methods:\n",
    "    if results[method]:\n",
    "        # Extract metrics for all runs\n",
    "        silhouettes = np.array([run[2] for run in results[method]])\n",
    "        sses = np.array([run[3] for run in results[method]])\n",
    "        quantization_errors = np.array([run[4] for run in results[method]])\n",
    "        \n",
    "        # Normalize metrics to [0,1] range\n",
    "        norm_silhouette = (silhouettes - np.min(silhouettes)) / (np.max(silhouettes) - np.min(silhouettes) + 1e-10)\n",
    "        norm_sse = (sses - np.min(sses)) / (np.max(sses) - np.min(sses) + 1e-10)\n",
    "        norm_quantization = (quantization_errors - np.min(quantization_errors)) / (np.max(quantization_errors) - np.min(quantization_errors) + 1e-10)\n",
    "        \n",
    "        # Combined score (higher is better)\n",
    "        combined_scores = norm_silhouette + (1 - norm_sse) + (1 - norm_quantization)\n",
    "        best_idx = np.argmax(combined_scores)\n",
    "        \n",
    "        # Store best results\n",
    "        labels_dict[method] = results[method][best_idx][0]\n",
    "        centroids_dict[method] = results[method][best_idx][1]\n",
    "        \n",
    "        # Print all metrics for best run\n",
    "        print(f\"\\n{method} - Best Run Metrics:\")\n",
    "        print(f\"Silhouette Score: {results[method][best_idx][2]:.4f}\")\n",
    "        print(f\"SSE: {results[method][best_idx][3]:.4f}\")\n",
    "        print(f\"Quantization Error: {results[method][best_idx][4]:.4f}\")\n",
    "    else:\n",
    "        print(f\"{method} failed to produce any valid clusterings.\")\n",
    "\n",
    "# Visualize best clusters for methods with valid results\n",
    "valid_methods = [method for method in methods if method in labels_dict]\n",
    "if valid_methods:\n",
    "    visualize_seed_clusters(x, valid_methods, labels_dict, centroids_dict)\n",
    "else:\n",
    "    print(\"No valid clustering results to visualize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette Study ðŸ“Š\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "def plot_enhanced_silhouette(data, labels_dict):\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(24, 18))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.suptitle(\"Silhouette Analysis of Clustering Methods\", fontsize=24, fontweight='bold', y=1.02)\n",
    "\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, 3))\n",
    "\n",
    "    for i, (method, labels) in enumerate(labels_dict.items()):\n",
    "        unique_labels = np.unique(labels)\n",
    "        if len(unique_labels) < 2:\n",
    "            axes[i].text(0.5, 0.5, f\"{method}\\nOnly one cluster found\", \n",
    "                         ha='center', va='center', fontsize=16)\n",
    "            axes[i].set_title(f'{method} - Invalid Clustering', fontsize=16, fontweight='bold')\n",
    "            continue\n",
    "\n",
    "        silhouette_vals = silhouette_samples(data, labels)\n",
    "        avg_score = silhouette_score(data, labels)\n",
    "        \n",
    "        y_lower, y_upper = 0, 0\n",
    "        yticks = []\n",
    "        for cluster in unique_labels:\n",
    "            cluster_silhouette_vals = silhouette_vals[labels == cluster]\n",
    "            cluster_silhouette_vals.sort()\n",
    "            size_cluster = len(cluster_silhouette_vals)\n",
    "            y_upper += size_cluster\n",
    "            \n",
    "            color = colors[cluster % len(colors)]\n",
    "            axes[i].fill_betweenx(np.arange(y_lower, y_upper), 0, cluster_silhouette_vals, \n",
    "                                  facecolor=color, edgecolor=color, alpha=0.7)\n",
    "            \n",
    "            yticks.append((y_lower + y_upper) / 2)\n",
    "            y_lower += size_cluster\n",
    "\n",
    "        axes[i].axvline(x=avg_score, color=\"red\", linestyle=\"--\", label='Average')\n",
    "        axes[i].set_title(f'Silhouette Plot - {method}', fontsize=16, fontweight='bold')\n",
    "        axes[i].set_xlabel('Silhouette coefficient', fontsize=12)\n",
    "        axes[i].set_ylabel('Cluster', fontsize=12)\n",
    "        axes[i].set_yticks(yticks)\n",
    "        axes[i].set_yticklabels(unique_labels)\n",
    "        axes[i].set_xlim([-0.1, 1])\n",
    "        axes[i].set_ylim(0, len(silhouette_vals))\n",
    "        \n",
    "        # Add text annotations\n",
    "        axes[i].text(0.05, 0.95, f'Average Score: {avg_score:.3f}', \n",
    "                     transform=axes[i].transAxes, fontsize=12, \n",
    "                     verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "        \n",
    "        # Add a legend\n",
    "        axes[i].legend(loc='lower right', fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Usage\n",
    "plot_enhanced_silhouette(x, labels_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE Analysis ðŸ”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def plot_enhanced_tsne(data, labels_dict, methods):\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    tsne_results = tsne.fit_transform(data)\n",
    "    \n",
    "    fig = plt.figure(figsize=(24, 18))\n",
    "    fig.suptitle(\"t-SNE Visualization of Clustering Methods\", fontsize=24, fontweight='bold', y=0.95)\n",
    "    \n",
    "    sns.set_style(\"whitegrid\")\n",
    "    sns.set_palette(\"deep\")\n",
    "    \n",
    "    for i, method in enumerate(methods):\n",
    "        ax = fig.add_subplot(2, 3, i+1)\n",
    "        \n",
    "        scatter = ax.scatter(tsne_results[:, 0], tsne_results[:, 1], \n",
    "                             c=labels_dict[method], cmap='viridis', \n",
    "                             alpha=0.7, s=50, edgecolors='w')\n",
    "        \n",
    "        ax.set_title(f'{method}', fontsize=18, fontweight='bold')\n",
    "        ax.set_xlabel('t-SNE feature 1', fontsize=12)\n",
    "        ax.set_ylabel('t-SNE feature 2', fontsize=12)\n",
    "        \n",
    "        # Add colorbar\n",
    "        cbar = plt.colorbar(scatter, ax=ax, aspect=40, pad=0.02)\n",
    "        cbar.set_label('Cluster', fontsize=12)\n",
    "        \n",
    "        # Add cluster centroids\n",
    "        unique_labels = np.unique(labels_dict[method])\n",
    "        centroids = np.array([np.mean(tsne_results[labels_dict[method] == label], axis=0) \n",
    "                              for label in unique_labels])\n",
    "        ax.scatter(centroids[:, 0], centroids[:, 1], \n",
    "                   marker='X', s=200, linewidths=2, \n",
    "                   c='red', edgecolors='k')\n",
    "        \n",
    "        # Add cluster labels\n",
    "        for label, centroid in zip(unique_labels, centroids):\n",
    "            ax.annotate(f'{label}', centroid, fontsize=12, fontweight='bold',\n",
    "                        ha='center', va='center',\n",
    "                        bbox=dict(boxstyle='circle,pad=0.5', fc='yellow', ec='k', alpha=0.7),\n",
    "                        xytext=(0, 0), textcoords='offset points')\n",
    "        \n",
    "        # Add legend for centroids\n",
    "        ax.scatter([], [], marker='X', s=200, linewidths=2, c='red', edgecolors='k', label='Centroids')\n",
    "        ax.legend(loc='best', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "# Usage\n",
    "plot_enhanced_tsne(x, labels_dict, methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Analysis ðŸ”¬\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "import re\n",
    "\n",
    "def plot_enhanced_pairplots(data, labels_dict, methods):\n",
    "    # Perform PCA\n",
    "    pca = PCA(n_components=min(3, data.shape[1]))\n",
    "    x_pca = pca.fit_transform(data)\n",
    "    df_pca = pd.DataFrame(x_pca, columns=[f'PC{i+1}' for i in range(x_pca.shape[1])])\n",
    "    \n",
    "    # Calculate explained variance\n",
    "    explained_variance = pca.explained_variance_ratio_ * 100\n",
    "\n",
    "    # Store the metrics for later plotting\n",
    "    metrics_dict = {method: {} for method in methods}\n",
    "\n",
    "    for method in methods:\n",
    "        df_pca[f'{method}_cluster'] = labels_dict[method]\n",
    "        \n",
    "        # Check if there's only one cluster\n",
    "        if len(np.unique(labels_dict[method])) < 2:\n",
    "            print(f\"Warning: {method} produced only one cluster. Skipping this method.\")\n",
    "            continue\n",
    "        \n",
    "        # Set up the plot style\n",
    "        sns.set(style=\"whitegrid\", font_scale=1.2)\n",
    "        colors = sns.color_palette(\"husl\", n_colors=len(np.unique(labels_dict[method])))\n",
    "        \n",
    "        # Create the pairplot\n",
    "        g = sns.pairplot(df_pca, hue=f'{method}_cluster', palette=colors, \n",
    "                         plot_kws={'alpha': 0.7, 's': 60, 'edgecolor': 'w'},\n",
    "                         diag_kws={'alpha': 0.5, 'edgecolor': 'w'},\n",
    "                         height=3, aspect=1.2)\n",
    "        \n",
    "        # Enhance the plot\n",
    "        g.fig.suptitle(f'Pairplot for {method}', fontsize=20, y=1.02)\n",
    "        \n",
    "        # Add explained variance to axis labels\n",
    "        for ax in g.axes.flat:\n",
    "            if ax is not None:\n",
    "                xlabel = ax.get_xlabel()\n",
    "                ylabel = ax.get_ylabel()\n",
    "                if xlabel:\n",
    "                    pc_num = re.search(r'PC(\\d+)', xlabel)\n",
    "                    if pc_num:\n",
    "                        pc_index = int(pc_num.group(1)) - 1\n",
    "                        ax.set_xlabel(f'{xlabel} ({explained_variance[pc_index]:.1f}%)')\n",
    "                if ylabel:\n",
    "                    pc_num = re.search(r'PC(\\d+)', ylabel)\n",
    "                    if pc_num:\n",
    "                        pc_index = int(pc_num.group(1)) - 1\n",
    "                        ax.set_ylabel(f'{ylabel} ({explained_variance[pc_index]:.1f}%)')\n",
    "        \n",
    "        # Adjust legend\n",
    "        g._legend.set_title('Clusters')\n",
    "        for t in g._legend.texts:\n",
    "            t.set_text(f'Cluster {t.get_text()}')\n",
    "        \n",
    "        # Calculate and store the metrics\n",
    "        try:\n",
    "            metrics_dict[method]['Silhouette Score'] = silhouette_score(data, labels_dict[method])\n",
    "            metrics_dict[method]['Calinski-Harabasz Index'] = calinski_harabasz_score(data, labels_dict[method])\n",
    "            metrics_dict[method]['Davies-Bouldin Index'] = davies_bouldin_score(data, labels_dict[method])\n",
    "            \n",
    "            metrics_text = (f\"Silhouette Score: {metrics_dict[method]['Silhouette Score']:.3f}\\n\"\n",
    "                            f\"Calinski-Harabasz Index: {metrics_dict[method]['Calinski-Harabasz Index']:.3f}\\n\"\n",
    "                            f\"Davies-Bouldin Index: {metrics_dict[method]['Davies-Bouldin Index']:.3f}\")\n",
    "        except ValueError as e:\n",
    "            metrics_text = f\"Error calculating metrics: {str(e)}\"\n",
    "        \n",
    "        plt.text(0.98, 0.05, metrics_text, transform=g.fig.transFigure, \n",
    "                 horizontalalignment='right', verticalalignment='bottom',\n",
    "                 bbox=dict(facecolor='white', alpha=0.8, edgecolor='gray', boxstyle='round,pad=0.5'))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return metrics_dict\n",
    "\n",
    "def plot_clustering_indices(metrics_dict):\n",
    "    indices = ['Davies-Bouldin Index', 'Calinski-Harabasz Index']\n",
    "    methods = list(metrics_dict.keys())\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    for i, index in enumerate(indices):\n",
    "        values = [metrics_dict[method][index] for method in methods]\n",
    "        \n",
    "        # Create the bar plot\n",
    "        sns.barplot(x=methods, y=values, ax=axes[i], capsize=0.1, color='skyblue', alpha=0.8)\n",
    "        \n",
    "        axes[i].set_title(f'Comparison of {index}')\n",
    "        axes[i].set_xlabel('Method')\n",
    "        axes[i].set_xticklabels(axes[i].get_xticklabels(), rotation=45, ha='right')\n",
    "        \n",
    "        # Add value labels at the top of each bar\n",
    "        for j, v in enumerate(values):\n",
    "            axes[i].text(j, v, f'{v:.3f}', ha='center', va='bottom')\n",
    "        \n",
    "        # Set y-axis label\n",
    "        axes[i].set_ylabel(index)\n",
    "        \n",
    "        # Adjust y-axis limits to make room for the labels at the top\n",
    "        y_max = max(values)\n",
    "        axes[i].set_ylim(0, y_max * 1.2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Usage\n",
    "# Assuming you have 'x' (your feature matrix), 'labels_dict' (dictionary of labels for each method),\n",
    "# and 'methods' (list of method names) already defined\n",
    "\n",
    "metrics_dict = plot_enhanced_pairplots(x, labels_dict, methods)\n",
    "plot_clustering_indices(metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "def plot_actual_silhouette(results_dict, benchmark_df):\n",
    "    # Prepare data\n",
    "    data = []\n",
    "    methods = ['K-Means++', 'PSO', 'PSO Hybrid', 'APSO', 'APSO Hybrid']\n",
    "    for method in methods:\n",
    "        if method == 'K-Means++':\n",
    "            values = benchmark_df[benchmark_df['method'] == method]['silhouette_mean'].tolist() * len(results_dict['PSO']['silhouette'])\n",
    "        else:\n",
    "            values = results_dict[method]['silhouette']\n",
    "        data.extend([(method, value) for value in values])\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=['Method', 'Value'])\n",
    "\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    # Create violin plot with custom color palette\n",
    "    sns.violinplot(x='Method', y='Value', data=df, \n",
    "                   palette='Set3', inner='box', cut=0)\n",
    "    \n",
    "    # Add swarm plot for individual points\n",
    "    sns.swarmplot(x='Method', y='Value', data=df, \n",
    "                  color='.3', size=3, alpha=0.7)\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.title('Silhouette Score Comparison Across Clustering Methods', \n",
    "              fontsize=22, fontweight='bold', pad=20)\n",
    "    plt.xlabel('Clustering Method', fontsize=18, labelpad=15)\n",
    "    plt.ylabel('Silhouette Score', fontsize=18, labelpad=15)\n",
    "    plt.xticks(rotation=30, ha='right', fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    \n",
    "    # Adjust y-axis\n",
    "    plt.ylim(df['Value'].min() * 0.95, df['Value'].max() * 1.05)\n",
    "    \n",
    "    # Add ANOVA results\n",
    "    f_value, p_value = stats.f_oneway(*(df[df['Method'] == method]['Value'] for method in methods))\n",
    "    anova_text = f\"ANOVA: F={f_value:.2f}, p={p_value:.4f}\"\n",
    "    plt.text(0.02, 0.98, anova_text, transform=plt.gca().transAxes, fontsize=14,\n",
    "             verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # Add explanation\n",
    "    explanation = \"Higher values indicate better-defined clusters.\"\n",
    "    plt.text(0.98, 0.98, explanation, transform=plt.gca().transAxes, fontsize=14,\n",
    "             verticalalignment='top', horizontalalignment='right',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n",
    "    \n",
    "    # Highlight the best method\n",
    "    best_method = df.groupby('Method')['Value'].mean().idxmax()\n",
    "    best_value = df.groupby('Method')['Value'].mean().max()\n",
    "    plt.text(0.02, 0.02, f\"Best Method: {best_method}\\nValue: {best_value:.4f}\", \n",
    "             transform=plt.gca().transAxes, fontsize=14,\n",
    "             verticalalignment='bottom', bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))\n",
    "    \n",
    "    # Add legend for violin plot parts\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    handles.extend([plt.Line2D([0], [0], color='w', marker='_', markersize=15, markerfacecolor='k', label='Median'),\n",
    "                    plt.Line2D([0], [0], color='k', marker='o', markersize=5, label='Individual Scores')])\n",
    "    plt.legend(handles=handles, title='Plot Elements', title_fontsize='14', fontsize='12', loc='upper left', bbox_to_anchor=(1, 1))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Usage\n",
    "plot_actual_silhouette({\n",
    "    'PSO': pso_plain,\n",
    "    'PSO Hybrid': pso_hybrid,\n",
    "    'APSO': apso_plain,\n",
    "    'APSO Hybrid': apso_hybrid\n",
    "}, benchmark_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Ranking ðŸ†\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_statistics(data):\n",
    "    return {\n",
    "        'mean': np.around(np.mean(data), decimals=10),\n",
    "        'stdev': np.around(np.std(data), decimals=10),\n",
    "        'min': np.around(np.min(data), decimals=10),\n",
    "        'max': np.around(np.max(data), decimals=10),\n",
    "        'percentile_75': np.around(np.percentile(data, 75), decimals=10),\n",
    "        'valid_count': np.sum(~np.isnan(data))\n",
    "    }\n",
    "\n",
    "def penalize_metric(value, success_rate, penalty_factor=1.0):\n",
    "    \"\"\"Penalize a metric based on success rate.\"\"\"\n",
    "    if np.isnan(value):\n",
    "        return np.nan\n",
    "    # For metrics where higher is better (e.g., silhouette)\n",
    "    if 'silhouette' in metric:\n",
    "        return value * (success_rate ** penalty_factor)\n",
    "    # For metrics where lower is better (e.g., SSE, quantization)\n",
    "    else:\n",
    "        return value / (success_rate ** penalty_factor)\n",
    "\n",
    "methods = ['K-Means++', 'PSO', 'PSO Hybrid', 'APSO', 'APSO Hybrid']\n",
    "metrics = ['sse', 'silhouette', 'quantization']\n",
    "results = [kmeanspp, pso_plain, pso_hybrid, apso_plain, apso_hybrid]\n",
    "\n",
    "benchmark = {'method': methods}\n",
    "\n",
    "for i, method in enumerate(methods):\n",
    "    valid_runs = np.sum(~np.isnan(results[i][metrics[0]]))\n",
    "    success_rate = valid_runs / 10  # 10 iterations\n",
    "    benchmark['success_rate'] = benchmark.get('success_rate', []) + [success_rate]\n",
    "    \n",
    "    for metric in metrics:\n",
    "        stats = calculate_statistics(results[i][metric])\n",
    "        for stat, value in stats.items():\n",
    "            if stat != 'valid_count':\n",
    "                penalized_value = penalize_metric(value, success_rate)\n",
    "                benchmark[f'{metric}_{stat}'] = benchmark.get(f'{metric}_{stat}', []) + [value]\n",
    "                benchmark[f'{metric}_{stat}_penalized'] = benchmark.get(f'{metric}_{stat}_penalized', []) + [penalized_value]\n",
    "\n",
    "benchmark_df = pd.DataFrame.from_dict(benchmark)\n",
    "\n",
    "# Reorder columns\n",
    "column_order = ['method', 'success_rate']\n",
    "for metric in metrics:\n",
    "    for stat in ['mean', 'stdev', 'min', 'max', 'percentile_75']:\n",
    "        column_order.extend([f'{metric}_{stat}', f'{metric}_{stat}_penalized'])\n",
    "\n",
    "benchmark_df = benchmark_df[column_order]\n",
    "\n",
    "print(benchmark_df.to_string())\n",
    "\n",
    "# Calculate overall score (example using penalized means)\n",
    "benchmark_df['overall_score'] = (\n",
    "    benchmark_df['silhouette_mean_penalized'] / benchmark_df['silhouette_mean_penalized'].max() -\n",
    "    benchmark_df['sse_mean_penalized'] / benchmark_df['sse_mean_penalized'].max() -\n",
    "    benchmark_df['quantization_mean_penalized'] / benchmark_df['quantization_mean_penalized'].max()\n",
    ")\n",
    "\n",
    "# Sort by overall score\n",
    "benchmark_df_sorted = benchmark_df.sort_values('overall_score', ascending=False)\n",
    "print(\"\\nRanked by Overall Score:\")\n",
    "print(benchmark_df_sorted[['method', 'success_rate', 'overall_score']].to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "def calculate_statistics(data):\n",
    "    return {\n",
    "        'mean': np.around(np.mean(data), decimals=10),\n",
    "        'stdev': np.around(np.std(data), decimals=10),\n",
    "        'min': np.around(np.min(data), decimals=10),\n",
    "        'max': np.around(np.max(data), decimals=10),\n",
    "        'percentile_75': np.around(np.percentile(data, 75), decimals=10),\n",
    "        'valid_count': np.sum(~np.isnan(data))\n",
    "    }\n",
    "\n",
    "def penalize_metric(value, success_rate, metric, penalty_factor=1.0):\n",
    "    \"\"\"Penalize a metric based on success rate.\"\"\"\n",
    "    if np.isnan(value):\n",
    "        return np.nan\n",
    "    # For metrics where higher is better\n",
    "    if metric in ['silhouette', 'calinski_harabasz']:\n",
    "        return value * (success_rate ** penalty_factor)\n",
    "    # For metrics where lower is better\n",
    "    else:  # ['sse', 'quantization', 'davies_bouldin']\n",
    "        return value / (success_rate ** penalty_factor)\n",
    "\n",
    "def calculate_validation_scores(x, labels):\n",
    "    \"\"\"Calculate clustering validation scores.\"\"\"\n",
    "    if len(np.unique(labels)) < 2:\n",
    "        return np.nan, np.nan, np.nan\n",
    "    \n",
    "    try:\n",
    "        sil_score = silhouette_score(x, labels)\n",
    "        ch_score = calinski_harabasz_score(x, labels)\n",
    "        db_score = davies_bouldin_score(x, labels)\n",
    "        return sil_score, ch_score, db_score\n",
    "    except:\n",
    "        return np.nan, np.nan, np.nan\n",
    "\n",
    "def calculate_comprehensive_score(df, dataset_name):\n",
    "    \"\"\"Calculate comprehensive score considering all metrics and validation indices.\"\"\"\n",
    "    metrics = ['silhouette', 'sse', 'quantization', 'calinski_harabasz', 'davies_bouldin']\n",
    "    stats = ['mean', 'percentile_75', 'max']\n",
    "    \n",
    "    # Initialize score components\n",
    "    score_components = {}\n",
    "    \n",
    "    # Calculate normalized scores for each metric and statistic\n",
    "    for metric in metrics:\n",
    "        for stat in stats:\n",
    "            col = f'{metric}_{stat}_penalized'\n",
    "            norm_col = f'{metric}_{stat}_normalized'\n",
    "            \n",
    "            # Skip if column doesn't exist\n",
    "            if col not in df.columns:\n",
    "                continue\n",
    "                \n",
    "            if metric in ['silhouette', 'calinski_harabasz']:\n",
    "                # Higher is better\n",
    "                score_components[norm_col] = df[col] / df[col].max()\n",
    "            else:\n",
    "                # Lower is better\n",
    "                score_components[norm_col] = 1 - (df[col] / df[col].max())\n",
    "    \n",
    "    # Create detailed scores DataFrame\n",
    "    detailed_scores = pd.DataFrame(score_components)\n",
    "    detailed_scores['method'] = df['method']\n",
    "    detailed_scores['success_rate'] = df['success_rate']\n",
    "    \n",
    "    # Weights for different components\n",
    "    metric_weights = {\n",
    "        'silhouette': 0.25,\n",
    "        'calinski_harabasz': 0.25,\n",
    "        'davies_bouldin': 0.2,\n",
    "        'sse': 0.15,\n",
    "        'quantization': 0.15\n",
    "    }\n",
    "    \n",
    "    stat_weights = {\n",
    "        'mean': 0.25,\n",
    "        'percentile_75': 0.5,\n",
    "        'max': 0.25\n",
    "    }\n",
    "    \n",
    "    # Calculate final score\n",
    "    final_score = np.zeros(len(df))\n",
    "    \n",
    "    for metric in metrics:\n",
    "        if f'{metric}_mean_normalized' not in score_components:\n",
    "            continue\n",
    "            \n",
    "        metric_score = 0\n",
    "        for stat in stats:\n",
    "            norm_col = f'{metric}_{stat}_normalized'\n",
    "            if norm_col in score_components:\n",
    "                metric_score += score_components[norm_col] * stat_weights[stat]\n",
    "        \n",
    "        final_score += metric_score * metric_weights[metric]\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    summary_cols = ['method', 'success_rate']\n",
    "    for metric in metrics:\n",
    "        if f'{metric}_mean' in df.columns:\n",
    "            summary_cols.extend([f'{metric}_mean', f'{metric}_mean_penalized'])\n",
    "    \n",
    "    summary_df = pd.DataFrame({\n",
    "        'dataset': dataset_name,\n",
    "        'method': df['method'],\n",
    "        'success_rate': df['success_rate'],\n",
    "        'comprehensive_score': final_score,\n",
    "        **{col: df[col] for col in summary_cols if col not in ['method', 'success_rate']}\n",
    "    })\n",
    "    \n",
    "    # Add rank\n",
    "    summary_df['rank'] = summary_df['comprehensive_score'].rank(ascending=False)\n",
    "    \n",
    "    return detailed_scores, summary_df\n",
    "\n",
    "def process_dataset(data, methods, x, dataset_name, penalty_factor=1.0):\n",
    "    \"\"\"Process a single dataset including validation indices.\"\"\"\n",
    "    metrics = ['sse', 'silhouette', 'quantization', 'calinski_harabasz', 'davies_bouldin']\n",
    "    benchmark = {'method': methods}\n",
    "    \n",
    "    for i, method in enumerate(methods):\n",
    "        valid_runs = np.sum(~np.isnan(data[i]['sse']))  # Using SSE for valid run count\n",
    "        success_rate = valid_runs / 10  # 10 iterations\n",
    "        benchmark['success_rate'] = benchmark.get('success_rate', []) + [success_rate]\n",
    "        \n",
    "        # Calculate validation scores for the best clustering\n",
    "        labels = labels_dict[method]\n",
    "        sil, ch, db = calculate_validation_scores(x, labels)\n",
    "        \n",
    "        # Add validation scores to the results\n",
    "        data[i]['calinski_harabasz'] = ch\n",
    "        data[i]['davies_bouldin'] = db\n",
    "        \n",
    "        for metric in metrics:\n",
    "            if metric in data[i]:\n",
    "                stats = calculate_statistics(data[i][metric])\n",
    "                for stat, value in stats.items():\n",
    "                    if stat != 'valid_count':\n",
    "                        penalized_value = penalize_metric(value, success_rate, metric, penalty_factor)\n",
    "                        benchmark[f'{metric}_{stat}'] = benchmark.get(f'{metric}_{stat}', []) + [value]\n",
    "                        benchmark[f'{metric}_{stat}_penalized'] = benchmark.get(f'{metric}_{stat}_penalized', []) + [penalized_value]\n",
    "    \n",
    "    benchmark_df = pd.DataFrame.from_dict(benchmark)\n",
    "    detailed_scores, summary_df = calculate_comprehensive_score(benchmark_df, dataset_name)\n",
    "    \n",
    "    return benchmark_df, detailed_scores, summary_df\n",
    "\n",
    "# Example usage\n",
    "benchmark_df, detailed_scores, summary_df = process_dataset(\n",
    "    results, methods, x, dataset_name=\"dataset1\"\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(\"\\nDetailed Scores:\")\n",
    "print(detailed_scores.to_string())\n",
    "print(\"\\nFinal Rankings:\")\n",
    "print(summary_df[['method', 'success_rate', 'comprehensive_score', 'rank']].sort_values('rank').to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Results ðŸ’¾\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "def calculate_statistics(data):\n",
    "    \"\"\"Calculate statistical measures for the data.\"\"\"\n",
    "    return {\n",
    "        'mean': np.around(np.mean(data), decimals=10),\n",
    "        'stdev': np.around(np.std(data), decimals=10),\n",
    "        'min': np.around(np.min(data), decimals=10),\n",
    "        'max': np.around(np.max(data), decimals=10),\n",
    "        'percentile_75': np.around(np.percentile(data, 75), decimals=10),\n",
    "        'valid_count': np.sum(~np.isnan(data))\n",
    "    }\n",
    "\n",
    "def penalize_metric(value, success_rate, metric, penalty_factor=1.0):\n",
    "    \"\"\"Penalize metric based on success rate and metric type.\"\"\"\n",
    "    if np.isnan(value):\n",
    "        return np.nan\n",
    "    if metric in ['silhouette', 'calinski_harabasz']:\n",
    "        return value * (success_rate ** penalty_factor)\n",
    "    else:  # ['sse', 'quantization', 'davies_bouldin']\n",
    "        return value / (success_rate ** penalty_factor)\n",
    "\n",
    "def calculate_validation_scores(x, labels):\n",
    "    \"\"\"Calculate clustering validation scores.\"\"\"\n",
    "    if len(np.unique(labels)) < 2:\n",
    "        return np.nan, np.nan, np.nan\n",
    "    try:\n",
    "        sil_score = silhouette_score(x, labels)\n",
    "        ch_score = calinski_harabasz_score(x, labels)\n",
    "        db_score = davies_bouldin_score(x, labels)\n",
    "        return sil_score, ch_score, db_score\n",
    "    except:\n",
    "        return np.nan, np.nan, np.nan\n",
    "\n",
    "def calculate_comprehensive_score(df, dataset_name):\n",
    "    \"\"\"Calculate comprehensive score considering all metrics and validation indices.\"\"\"\n",
    "    metrics = ['silhouette', 'sse', 'quantization', 'calinski_harabasz', 'davies_bouldin']\n",
    "    stats = ['mean', 'percentile_75', 'max']\n",
    "    \n",
    "    # Initialize score components\n",
    "    score_components = {}\n",
    "    \n",
    "    # Calculate normalized scores for each metric and statistic\n",
    "    for metric in metrics:\n",
    "        for stat in stats:\n",
    "            col = f'{metric}_{stat}_penalized'\n",
    "            norm_col = f'{metric}_{stat}_normalized'\n",
    "            \n",
    "            if col not in df.columns:\n",
    "                continue\n",
    "                \n",
    "            if metric in ['silhouette', 'calinski_harabasz']:\n",
    "                # Higher is better\n",
    "                score_components[norm_col] = df[col] / df[col].max()\n",
    "            else:\n",
    "                # Lower is better - use min/value so smaller values get higher scores\n",
    "                score_components[norm_col] = df[col].min() / df[col]\n",
    "    \n",
    "    # Create detailed scores DataFrame\n",
    "    detailed_scores = pd.DataFrame(score_components)\n",
    "    detailed_scores['method'] = df['method']\n",
    "    detailed_scores['success_rate'] = df['success_rate']\n",
    "    \n",
    "    # Weights for different components\n",
    "    metric_weights = {\n",
    "        'silhouette': 0.25,\n",
    "        'calinski_harabasz': 0.25,\n",
    "        'davies_bouldin': 0.2,\n",
    "        'sse': 0.15,\n",
    "        'quantization': 0.15\n",
    "    }\n",
    "    \n",
    "    stat_weights = {\n",
    "        'mean': 0.25,\n",
    "        'percentile_75': 0.5,\n",
    "        'max': 0.25\n",
    "    }\n",
    "    \n",
    "    # Calculate final score\n",
    "    final_score = np.zeros(len(df))\n",
    "    \n",
    "    for metric in metrics:\n",
    "        if f'{metric}_mean_normalized' not in score_components:\n",
    "            continue\n",
    "            \n",
    "        metric_score = 0\n",
    "        for stat in stats:\n",
    "            norm_col = f'{metric}_{stat}_normalized'\n",
    "            if norm_col in score_components:\n",
    "                metric_score += score_components[norm_col] * stat_weights[stat]\n",
    "        \n",
    "        final_score += metric_score * metric_weights[metric]\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    summary_cols = ['method', 'success_rate']\n",
    "    for metric in metrics:\n",
    "        if f'{metric}_mean' in df.columns:\n",
    "            summary_cols.extend([f'{metric}_mean', f'{metric}_mean_penalized'])\n",
    "    \n",
    "    summary_df = pd.DataFrame({\n",
    "        'dataset': dataset_name,\n",
    "        'method': df['method'],\n",
    "        'success_rate': df['success_rate'],\n",
    "        'comprehensive_score': final_score,\n",
    "        **{col: df[col] for col in summary_cols if col not in ['method', 'success_rate']}\n",
    "    })\n",
    "    \n",
    "    # Add rank\n",
    "    summary_df['rank'] = summary_df['comprehensive_score'].rank(ascending=False)\n",
    "    \n",
    "    return detailed_scores, summary_df\n",
    "\n",
    "def process_dataset(data, methods, x, dataset_name, penalty_factor=1.0):\n",
    "    \"\"\"Process a single dataset including validation indices.\"\"\"\n",
    "    metrics = ['sse', 'silhouette', 'quantization', 'calinski_harabasz', 'davies_bouldin']\n",
    "    benchmark = {'method': methods}\n",
    "    \n",
    "    for i, method in enumerate(methods):\n",
    "        valid_runs = np.sum(~np.isnan(data[i]['sse']))  # Using SSE for valid run count\n",
    "        success_rate = valid_runs / 10  # 10 iterations\n",
    "        benchmark['success_rate'] = benchmark.get('success_rate', []) + [success_rate]\n",
    "        \n",
    "        # Calculate validation scores for the best clustering\n",
    "        labels = labels_dict[method]\n",
    "        sil, ch, db = calculate_validation_scores(x, labels)\n",
    "        \n",
    "        # Add validation scores to the results\n",
    "        data[i]['calinski_harabasz'] = ch\n",
    "        data[i]['davies_bouldin'] = db\n",
    "        \n",
    "        for metric in metrics:\n",
    "            if metric in data[i]:\n",
    "                stats = calculate_statistics(data[i][metric])\n",
    "                for stat, value in stats.items():\n",
    "                    if stat != 'valid_count':\n",
    "                        penalized_value = penalize_metric(value, success_rate, metric, penalty_factor)\n",
    "                        benchmark[f'{metric}_{stat}'] = benchmark.get(f'{metric}_{stat}', []) + [value]\n",
    "                        benchmark[f'{metric}_{stat}_penalized'] = benchmark.get(f'{metric}_{stat}_penalized', []) + [penalized_value]\n",
    "    \n",
    "    benchmark_df = pd.DataFrame.from_dict(benchmark)\n",
    "    detailed_scores, summary_df = calculate_comprehensive_score(benchmark_df, dataset_name)\n",
    "    \n",
    "    return benchmark_df, detailed_scores, summary_df\n",
    "\n",
    "def export_dataset_analysis(results, methods, x, dataset_name, main_csv='all_clustering_results.csv'):\n",
    "    \"\"\"\n",
    "    Export dataset analysis results and append to main CSV.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results : list\n",
    "        Clustering results for each method\n",
    "    methods : list\n",
    "        List of clustering methods\n",
    "    x : array-like\n",
    "        Dataset being analyzed\n",
    "    dataset_name : str\n",
    "        Name of the dataset\n",
    "    main_csv : str\n",
    "        Name of the main CSV file to store all results\n",
    "    \"\"\"\n",
    "    # Process dataset\n",
    "    benchmark_df, detailed_scores, summary_df = process_dataset(results, methods, x, dataset_name)\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame()\n",
    "    results_df['Dataset'] = [str(dataset_name)] * len(methods)  # Ensure dataset name is string\n",
    "    results_df['Method'] = methods\n",
    "    results_df['Success_Rate'] = benchmark_df['success_rate']\n",
    "    results_df['Final_Score'] = summary_df['comprehensive_score']\n",
    "    results_df['Rank'] = summary_df['rank']\n",
    "    \n",
    "    # Add all metrics\n",
    "    metrics = ['silhouette', 'calinski_harabasz', 'davies_bouldin', 'sse', 'quantization']\n",
    "    stats = ['mean', 'percentile_75', 'max']\n",
    "    \n",
    "    for metric in metrics:\n",
    "        for stat in stats:\n",
    "            col = f'{metric}_{stat}'\n",
    "            if col in benchmark_df.columns:\n",
    "                results_df[f'{metric}_{stat}'] = benchmark_df[col]\n",
    "                if f'{col}_penalized' in benchmark_df.columns:\n",
    "                    results_df[f'{metric}_{stat}_penalized'] = benchmark_df[f'{col}_penalized']\n",
    "    \n",
    "    # Add normalized scores\n",
    "    for metric in metrics:\n",
    "        for stat in stats:\n",
    "            norm_col = f'{metric}_{stat}_normalized'\n",
    "            if norm_col in detailed_scores.columns:\n",
    "                results_df[norm_col] = detailed_scores[norm_col]\n",
    "    \n",
    "    # Handle main CSV file\n",
    "    try:\n",
    "        main_df = pd.read_csv(main_csv)\n",
    "        # Convert Dataset column to string type\n",
    "        main_df['Dataset'] = main_df['Dataset'].astype(str)\n",
    "        # Remove previous results for this dataset if they exist\n",
    "        main_df = main_df[main_df['Dataset'] != str(dataset_name)]\n",
    "        # Append new results\n",
    "        main_df = pd.concat([main_df, results_df], ignore_index=True)\n",
    "    except FileNotFoundError:\n",
    "        main_df = results_df\n",
    "    \n",
    "    # Save to main CSV\n",
    "    main_df.to_csv(main_csv, index=False)\n",
    "    \n",
    "    # Print simple summary\n",
    "    print(f\"\\nResults for {dataset_name} dataset added to {main_csv}\")\n",
    "    unique_datasets = sorted(main_df['Dataset'].unique().astype(str))\n",
    "    print(f\"Datasets in main CSV: {', '.join(unique_datasets)}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "\n",
    "export_dataset_analysis(results, methods, x, \"Iris\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
